{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n",
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jacop\\onedrive\\desktop\\thesis\\data analysis\\statistics\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install re\n",
    "!pip install os\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High density data processed and saved.\n",
      "No Low density data found. Skipping low density processing.\n"
     ]
    }
   ],
   "source": [
    "'''PART 1: COMBINE OUTPUT SIMULATIONS IN A BIGGER DATASET FOR ANALYSIS.'''\n",
    "\n",
    "import pandas as pd\n",
    "import re  \n",
    "import os\n",
    "\n",
    "# Path to the folder containing your files\n",
    "folder_path = 'output_model/'\n",
    "\n",
    "# List to store all file paths\n",
    "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize two lists to store dataframes by density\n",
    "dfs_high = []\n",
    "dfs_low = []\n",
    "\n",
    "# Function to extract repetition, density, and architecture from the filename\n",
    "def extract_simulation_info(file_path):\n",
    "    match = re.search(r'repetition_(\\d+)_(High|Low)_(architecture_[A-I]|control)', file_path)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2), match.group(3)  # repetition, density, architecture\n",
    "    else:\n",
    "        return 'Unknown', 'Unknown', 'Unknown'\n",
    "\n",
    "# Loop through each file\n",
    "for file in file_paths:\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Adjust the 'rank' column by subtracting 1 from each value to start ranks from 1\n",
    "    df['rank'] = df['rank'] - 1\n",
    "\n",
    "    # Remove rows where Organ_ID starts with \"organs.Internode\"\n",
    "    df = df[~df['Organ_ID'].str.startswith('organs.Internode')]\n",
    "\n",
    "    # Extract repetition, density, and architecture from the filename\n",
    "    repetition, density, architecture = extract_simulation_info(file)\n",
    "\n",
    "    # Add the extracted information as new columns\n",
    "    df['density'] = density\n",
    "    df['architecture'] = architecture\n",
    "    df['repetition'] = repetition  \n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'organ_type', 'age_in_degree_days_dd of plant', 'age_in_days_d of plant',\n",
    "        'order', 'dry_biomass_mg[mg]', 'dry_biomass_growth_mg for dynamic', 'SLA'\n",
    "    ]\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Create new column for absorbedPAR [umol m^-2 s^-1]\n",
    "    df['absorbedPAR_umol_m2_s1'] = df['absorbedPAR [umol s^-1]'] / df['area_m2[m^2]']\n",
    "\n",
    "    # Handle rank 1 separately by first summing the values for two leaves and then averaging\n",
    "    rank1_df = df[df['rank'] == 1]\n",
    "    numeric_columns = ['absorbedPAR_umol_m2_s1', 'area_m2[m^2]', 'absorbedPAR [umol s^-1]']\n",
    "    rank1_summed = rank1_df.groupby(['density', 'architecture', 'repetition', 'plantNb']).sum().reset_index()\n",
    "    rank1_averaged = rank1_summed.groupby(['density', 'architecture', 'repetition'])[numeric_columns].mean().reset_index()\n",
    "    rank1_averaged['rank'] = 1 \n",
    "    \n",
    "    # Handle other ranks normally\n",
    "    other_ranks_df = df[df['rank'] != 1]\n",
    "    other_ranks_mean = other_ranks_df.groupby(['density', 'architecture', 'repetition', 'rank'])[numeric_columns].mean().reset_index()\n",
    "\n",
    "    # Combine rank 2 and other ranks\n",
    "    combined_df = pd.concat([rank1_averaged, other_ranks_mean], ignore_index=True)\n",
    "    \n",
    "    # Append the processed DataFrame to the appropriate list based on density\n",
    "    if density == 'High':\n",
    "        dfs_high.append(combined_df)\n",
    "    elif density == 'Low':\n",
    "        dfs_low.append(combined_df)\n",
    "\n",
    "# Concatenate all High density dataframes into one\n",
    "if dfs_high:  # Check if there are any dataframes to concatenate\n",
    "    combined_high = pd.concat(dfs_high, ignore_index=True)\n",
    "    combined_high = combined_high[['density', 'architecture', 'repetition', 'rank', 'area_m2[m^2]', 'absorbedPAR_umol_m2_s1']]\n",
    "    combined_high.to_csv('combined_files/combined_high_ranks.csv', index=False)\n",
    "    print(\"High density data processed and saved.\")\n",
    "else:\n",
    "    print(\"No High density data found. Skipping high density processing.\")\n",
    "\n",
    "# Concatenate all Low density dataframes into one\n",
    "if dfs_low:  # Check if there are any dataframes to concatenate\n",
    "    combined_low = pd.concat(dfs_low, ignore_index=True)\n",
    "    combined_low = combined_low[['density', 'architecture', 'repetition', 'rank', 'area_m2[m^2]', 'absorbedPAR_umol_m2_s1']]\n",
    "    combined_low.to_csv('combined_files/combined_low_ranks.csv', index=False)\n",
    "    print(\"Low density data processed and saved.\")\n",
    "else:\n",
    "    print(\"No Low density data found. Skipping low density processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation complete. Data saved to: combined_files/combined_total_absorbedPAR_high.csv\n",
      "Aggregation complete. Data saved to: combined_files/combined_total_absorbedPAR_low.csv\n"
     ]
    }
   ],
   "source": [
    "'''PART 2: CREATE DATASET FOR TOTAL ABSORBED PAR FOR EACH PLANT IN HIGH AND LOW DENSITY.'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to process and aggregate the data\n",
    "def aggregate_absorbed_PAR(file_path, output_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Group the data by 'density', 'architecture', 'repetition' and sum the specified columns\n",
    "    grouped_data = data.groupby(['density', 'architecture', 'repetition']).agg({\n",
    "        'absorbedPAR_umol_m2_s1': 'sum',\n",
    "        'area_m2[m^2]': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Save the aggregated data to a new CSV file\n",
    "    grouped_data.to_csv(output_path, index=False)\n",
    "    print(f\"Aggregation complete. Data saved to: {output_path}\")\n",
    "\n",
    "# File paths for high and low density data\n",
    "file_paths = {\n",
    "    'high': ('combined_files/combined_high_ranks.csv', 'combined_files/combined_total_absorbedPAR_high.csv'),\n",
    "    'low': ('combined_files/combined_low_ranks.csv', 'combined_files/combined_total_absorbedPAR_low.csv')\n",
    "}\n",
    "\n",
    "# Loop through both high and low files and apply the aggregation\n",
    "for density, (input_file, output_file) in file_paths.items():\n",
    "    aggregate_absorbed_PAR(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High density data processed and saved.\n",
      "No Low density data to process. Skipping low density processing.\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "'''PART 3: COMBINE RESULTS SENSORS FROM OUTPUT SIMULATION IN A BIGGER DATASET.'''\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Specify folder path and create a list of all csv files contained in the folder path.\n",
    "folder_path = \"output_sensors/\"\n",
    "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize two lists to store dataframes by density\n",
    "dfs_high = []\n",
    "dfs_low = []\n",
    "\n",
    "# Function to extract repetition, density, and architecture from the filename\n",
    "def extract_simulation_info(file_path):\n",
    "    match = re.search(r'sensors_export_below_canopy\\.csv_repetition_(\\d+)_(High|Low)_(architecture_[A-I]|control)', file_path)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2), match.group(3)  # repetition, density, architecture\n",
    "    else:\n",
    "        return 'Unknown', 'Unknown', 'Unknown'\n",
    "    \n",
    "# Loop through each file.\n",
    "for file in file_paths:\n",
    "    # Load the file into a pandas dataframe.\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Calculate mean of all rows for the specified numeric columns\n",
    "    numeric_columns = [' Tile z cohordinate [m]', 'absorbedPARTile [umol^s-1]']\n",
    "    mean_values = df[numeric_columns].mean()  # Mean values for each numeric column\n",
    "    \n",
    "    # Convert mean_values to a DataFrame to keep a single row\n",
    "    mean_df = pd.DataFrame([mean_values])\n",
    "\n",
    "    # Extract repetition, density, and architecture from the filename\n",
    "    repetition, density, architecture = extract_simulation_info(file)\n",
    "\n",
    "    # Add the extracted information as new columns\n",
    "    mean_df['density'] = density\n",
    "    mean_df['architecture'] = architecture\n",
    "    mean_df['repetition'] = repetition  \n",
    "    \n",
    "    # Calculate the new column for absorbedPAR micromols m-2 s-1 (using mean values)\n",
    "    mean_df['absorbedPAR_umol_m2_s1'] = mean_df['absorbedPARTile [umol^s-1]'] / mean_df[' Tile z cohordinate [m]']\n",
    "    \n",
    "    # Drop unnecessary columns from the mean_df\n",
    "    mean_df.drop(columns=[' Tile z cohordinate [m]', 'absorbedPARTile [umol^s-1]'], inplace=True)\n",
    "    \n",
    "    # Append the processed DataFrame to the appropriate list based on density\n",
    "    if density == 'High':\n",
    "        dfs_high.append(mean_df)\n",
    "    elif density == 'Low':\n",
    "        dfs_low.append(mean_df)\n",
    "\n",
    "# Concatenate all high and low density dataframes if they are not empty\n",
    "if dfs_high:  # Check if there are any dataframes in the high density list\n",
    "    combined_high = pd.concat(dfs_high, ignore_index=True)\n",
    "    combined_high = combined_high[['density', 'architecture', 'repetition', 'absorbedPAR_umol_m2_s1']]\n",
    "    # Save the combined high density data to a new CSV\n",
    "    combined_high.to_csv('combined_files/combined_high_sensors.csv', index=False)\n",
    "    print(\"High density data processed and saved.\")\n",
    "else:\n",
    "    print(\"No High density data to process. Skipping high density processing.\")\n",
    "\n",
    "if dfs_low:  # Check if there are any dataframes in the low density list\n",
    "    combined_low = pd.concat(dfs_low, ignore_index=True)\n",
    "    combined_low = combined_low[['density', 'architecture', 'repetition', 'absorbedPAR_umol_m2_s1']]\n",
    "    # Save the combined low density data to a new CSV\n",
    "    combined_low.to_csv('combined_files/combined_low_sensors.csv', index=False)\n",
    "    print(\"Low density data processed and saved.\")\n",
    "else:\n",
    "    print(\"No Low density data to process. Skipping low density processing.\")\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers removed and saved to 'combined_files/combined_high_ranks_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_high_ranks_cleaned.csv'\n",
      "Boxplots with outliers saved in 'boxplots/high_ranks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacop\\OneDrive\\Desktop\\Thesis\\Data analysis\\Statistics\\venv\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:233: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(**fig_kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned boxplots saved in 'boxplots/high_ranks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacop\\AppData\\Local\\Temp\\ipykernel_11304\\1035860870.py:48: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers removed and saved to 'combined_files/combined_low_ranks_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_low_ranks_cleaned.csv'\n",
      "Boxplots with outliers saved in 'boxplots/low_ranks'\n",
      "Cleaned boxplots saved in 'boxplots/low_ranks'\n",
      "Processing complete for all datasets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''PART 4: CHECK AND ELIMINATE OUTLIERS, CREATE BOXPLOT OF DATA FOR RANKS (ORIGINAL AND CLEANED).'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to process data, detect outliers, generate boxplots, and save results\n",
    "def process_ranks_data(input_file, output_cleaned_file, output_outliers_file, output_boxplot_folder, density_label):\n",
    "    # Create folder for boxplots if it doesn't exist\n",
    "    if not os.path.exists(output_boxplot_folder):\n",
    "        os.makedirs(output_boxplot_folder)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Initialize empty DataFrames to store cleaned data and outliers\n",
    "    cleaned_data = pd.DataFrame()\n",
    "    outliers = pd.DataFrame()\n",
    "\n",
    "    # Process data by rank\n",
    "    for rank in data['rank'].unique():\n",
    "        # Filter data for the current rank\n",
    "        rank_data = data[data['rank'] == rank]\n",
    "        \n",
    "        # Detect and remove outliers for each architecture within this rank\n",
    "        for architecture in rank_data['architecture'].unique():\n",
    "            # Filter data for the current architecture within the current rank\n",
    "            arch_data = rank_data[rank_data['architecture'] == architecture]\n",
    "\n",
    "            # Calculate Q1, Q3, and IQR\n",
    "            multiplier = 1.5\n",
    "            Q1 = arch_data['absorbedPAR_umol_m2_s1'].quantile(0.25)\n",
    "            Q3 = arch_data['absorbedPAR_umol_m2_s1'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - multiplier * IQR\n",
    "            upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "            # Identify and remove outliers\n",
    "            is_outlier = (arch_data['absorbedPAR_umol_m2_s1'] < lower_bound) | (arch_data['absorbedPAR_umol_m2_s1'] > upper_bound)\n",
    "            arch_outliers = arch_data[is_outlier]\n",
    "            arch_data_cleaned = arch_data[~is_outlier]\n",
    "\n",
    "            # Append the cleaned data and outliers\n",
    "            cleaned_data = pd.concat([cleaned_data, arch_data_cleaned], ignore_index=True)\n",
    "            outliers = pd.concat([outliers, arch_outliers], ignore_index=True)\n",
    "\n",
    "        # Generate and save boxplot with outliers for the current rank\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        rank_data.boxplot(column='absorbedPAR_umol_m2_s1', by='architecture', grid=False)\n",
    "        plt.title(f'Boxplot of Absorbed PAR by Architecture for Rank {rank} ({density_label})')\n",
    "        plt.suptitle('')\n",
    "        plt.xticks(fontsize=5)\n",
    "        plt.ylabel('Absorbed PAR (umol m-2 s-1)')\n",
    "        plt.savefig(os.path.join(output_boxplot_folder, f'{density_label}_rank_{rank}_with_outliers.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # Save the cleaned data and outliers to CSV files\n",
    "    cleaned_data.to_csv(output_cleaned_file, index=False)\n",
    "    outliers.to_csv(output_outliers_file, index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Outliers removed and saved to '{output_outliers_file}'\")\n",
    "    print(f\"Cleaned data saved to '{output_cleaned_file}'\")\n",
    "    print(f\"Boxplots with outliers saved in '{output_boxplot_folder}'\")\n",
    "\n",
    "    # Generate and save boxplots for cleaned data\n",
    "    for rank in cleaned_data['rank'].unique():\n",
    "        rank_data_cleaned = cleaned_data[cleaned_data['rank'] == rank]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        rank_data_cleaned.boxplot(column='absorbedPAR_umol_m2_s1', by='architecture', grid=False)\n",
    "        plt.title(f'Cleaned Boxplot of Absorbed PAR by Architecture for Rank {rank} ({density_label})')\n",
    "        plt.suptitle('')\n",
    "        plt.xticks(fontsize=5)\n",
    "        plt.ylabel('Absorbed PAR (umol m-2 s-1)')\n",
    "        plt.savefig(os.path.join(output_boxplot_folder, f'{density_label}_rank_{rank}_cleaned.png'))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Cleaned boxplots saved in '{output_boxplot_folder}'\")\n",
    "\n",
    "\n",
    "# Define file paths and labels for high and low density datasets\n",
    "datasets = [\n",
    "    {\n",
    "        'input_file': 'combined_files/combined_high_ranks.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_high_ranks_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_high_ranks_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/high_ranks',\n",
    "        'label': 'High Density'\n",
    "    },\n",
    "    {\n",
    "        'input_file': 'combined_files/combined_low_ranks.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_low_ranks_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_low_ranks_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/low_ranks',\n",
    "        'label': 'Low Density'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each dataset using the function\n",
    "for dataset in datasets:\n",
    "    process_ranks_data(\n",
    "        dataset['input_file'],\n",
    "        dataset['output_cleaned_file'],\n",
    "        dataset['output_outliers_file'],\n",
    "        dataset['output_boxplot_folder'],\n",
    "        dataset['label']\n",
    "    )\n",
    "\n",
    "print(\"Processing complete for all datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected and saved to 'combined_files/combined_high_sensors_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_high_sensors_cleaned.csv'\n",
      "Boxplots saved in 'boxplots/high_sensors'\n",
      "Outliers detected and saved to 'combined_files/combined_low_sensors_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_low_sensors_cleaned.csv'\n",
      "Boxplots saved in 'boxplots/low_sensors'\n",
      "Outliers detected and saved to 'combined_files/combined_total_absorbedPAR_high_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_total_absorbedPAR_high_cleaned.csv'\n",
      "Boxplots saved in 'boxplots/total_absorbedPAR_high'\n",
      "Outliers detected and saved to 'combined_files/combined_total_absorbedPAR_low_outliers.csv'\n",
      "Cleaned data saved to 'combined_files/combined_total_absorbedPAR_low_cleaned.csv'\n",
      "Boxplots saved in 'boxplots/total_absorbedPAR_low'\n",
      "Processing complete for all datasets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''PART 5: OUTLIERS CHECK AND BOXPLOT GENERATION FOR ORIGINAL AND CLEANED DATA OF SENSORS AND TOTAL ABSORBED PAR PER PLANT.'''\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to detect outliers, generate boxplots (original and cleaned data), and save results\n",
    "def handle_outliers_and_boxplots(input_file, output_cleaned_file, output_outliers_file, output_boxplot_folder, dataset_label):\n",
    "    # Create folder for boxplots if it doesn't exist\n",
    "    if not os.path.exists(output_boxplot_folder):\n",
    "        os.makedirs(output_boxplot_folder)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Initialize empty DataFrames to store cleaned data and outliers\n",
    "    cleaned_data = pd.DataFrame()\n",
    "    outliers = pd.DataFrame()\n",
    "\n",
    "    # Detect and remove outliers for each architecture\n",
    "    for architecture in data['architecture'].unique():\n",
    "        # Filter data for the current architecture\n",
    "        arch_data = data[data['architecture'] == architecture]\n",
    "\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        multiplier = 1.5\n",
    "        Q1 = arch_data['absorbedPAR_umol_m2_s1'].quantile(0.25)\n",
    "        Q3 = arch_data['absorbedPAR_umol_m2_s1'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "        # Identify outliers and filter them\n",
    "        is_outlier = (arch_data['absorbedPAR_umol_m2_s1'] < lower_bound) | (arch_data['absorbedPAR_umol_m2_s1'] > upper_bound)\n",
    "        arch_outliers = arch_data[is_outlier]\n",
    "        arch_data_cleaned = arch_data[~is_outlier]\n",
    "\n",
    "        # Append the cleaned data and outliers\n",
    "        cleaned_data = pd.concat([cleaned_data, arch_data_cleaned], ignore_index=True)\n",
    "        outliers = pd.concat([outliers, arch_outliers], ignore_index=True)\n",
    "\n",
    "    # Generate and save original boxplot (before outlier removal)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data.boxplot(column='absorbedPAR_umol_m2_s1', by='architecture', grid=False)\n",
    "    plt.title(f'Original Boxplot of Absorbed PAR ({dataset_label})')\n",
    "    plt.suptitle('')\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.ylabel('Absorbed PAR (umol m-2 s-1)')\n",
    "    plt.savefig(os.path.join(output_boxplot_folder, f'{dataset_label}_original.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Generate and save cleaned boxplot (after outlier removal)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cleaned_data.boxplot(column='absorbedPAR_umol_m2_s1', by='architecture', grid=False)\n",
    "    plt.title(f'Cleaned Boxplot of Absorbed PAR ({dataset_label})')\n",
    "    plt.suptitle('')\n",
    "    plt.xticks(fontsize=5)\n",
    "    plt.ylabel('Absorbed PAR (umol m-2 s-1)')\n",
    "    plt.savefig(os.path.join(output_boxplot_folder, f'{dataset_label}_cleaned.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save the cleaned data and outliers\n",
    "    cleaned_data.to_csv(output_cleaned_file, index=False)\n",
    "    outliers.to_csv(output_outliers_file, index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Outliers detected and saved to '{output_outliers_file}'\")\n",
    "    print(f\"Cleaned data saved to '{output_cleaned_file}'\")\n",
    "    print(f\"Boxplots saved in '{output_boxplot_folder}'\")\n",
    "\n",
    "\n",
    "# Define file paths and labels for high and low density datasets\n",
    "datasets = [\n",
    "    {\n",
    "        'input_file': 'combined_files/combined_high_sensors.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_high_sensors_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_high_sensors_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/high_sensors',\n",
    "        'label': 'High Density'\n",
    "    },\n",
    "    {\n",
    "        'input_file': 'combined_files/combined_low_sensors.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_low_sensors_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_low_sensors_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/low_sensors',\n",
    "        'label': 'Low Density'\n",
    "    },\n",
    "        {\n",
    "        'input_file': 'combined_files/combined_total_absorbedPAR_high.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_total_absorbedPAR_high_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_total_absorbedPAR_high_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/total_absorbedPAR_high',\n",
    "        'label': 'High Density'\n",
    "    },\n",
    "    {\n",
    "        'input_file': 'combined_files/combined_total_absorbedPAR_low.csv',\n",
    "        'output_cleaned_file': 'combined_files/combined_total_absorbedPAR_low_cleaned.csv',\n",
    "        'output_outliers_file': 'combined_files/combined_total_absorbedPAR_low_outliers.csv',\n",
    "        'output_boxplot_folder': 'boxplots/total_absorbedPAR_low',\n",
    "        'label': 'Low Density'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each dataset using the function\n",
    "for dataset in datasets:\n",
    "    handle_outliers_and_boxplots(\n",
    "        dataset['input_file'],\n",
    "        dataset['output_cleaned_file'],\n",
    "        dataset['output_outliers_file'],\n",
    "        dataset['output_boxplot_folder'],\n",
    "        dataset['label']\n",
    "    )\n",
    "\n",
    "print(\"Processing complete for all datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation applied and saved for combined_high_ranks.csv\n",
      "Log transformation applied and saved for combined_high_ranks_cleaned.csv\n",
      "Log transformation applied and saved for combined_high_ranks_outliers.csv\n",
      "Log transformation applied and saved for combined_high_sensors.csv\n",
      "Log transformation applied and saved for combined_high_sensors_cleaned.csv\n",
      "Log transformation applied and saved for combined_high_sensors_outliers.csv\n",
      "Log transformation applied and saved for combined_low_ranks.csv\n",
      "Log transformation applied and saved for combined_low_ranks_cleaned.csv\n",
      "Log transformation applied and saved for combined_low_ranks_outliers.csv\n",
      "Log transformation applied and saved for combined_low_sensors.csv\n",
      "Log transformation applied and saved for combined_low_sensors_cleaned.csv\n",
      "Log transformation applied and saved for combined_low_sensors_outliers.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_high.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_high_cleaned.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_high_outliers.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_low.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_low_cleaned.csv\n",
      "Log transformation applied and saved for combined_total_absorbedPAR_low_outliers.csv\n",
      "All files processed and saved in 'log_transformed' folder.\n"
     ]
    }
   ],
   "source": [
    "'''PART 6: LOG TRANFORMATION OF DATA.'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder = 'combined_files/'\n",
    "output_folder = 'log_transformed/'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Loop through the files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Load the CSV file\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Log transformation (base 10) of the 'absorbedPAR_umol_m2_s1' column\n",
    "        if 'absorbedPAR_umol_m2_s1' in data.columns:\n",
    "            # Apply log transformation\n",
    "            data['log_absorbedPAR_umol_m2_s1'] = np.log10(data['absorbedPAR_umol_m2_s1'])\n",
    "\n",
    "            # Save the transformed data to a new file in the output folder\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            data.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Log transformation applied and saved for {file_name}\")\n",
    "\n",
    "print(\"All files processed and saved in 'log_transformed' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
